{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2:**\n",
        "\n",
        "We switched to AdamW optimiser with a learning rate of 0.001 (1e-3) and a weight decay of 0.0001 (1e-4). The momentum is not specified because the AdamW optimizer does not use momentum as a parameter. The AdamW optimizer is an improved version of the original Adam optimizer, which incorporates L2 regularization (weight decay).\n",
        "\n",
        "To summarize, our optimizer was AdamW with the following parameters:\n",
        "\n",
        "Learning rate (lr): 0.001 & Weight decay: 0.0001\n",
        "\n"
      ],
      "metadata": {
        "id": "4AupEA_oqUHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIdcu81CDnMA",
        "outputId": "90d43cc7-3cc5-47f0-e117-5e7b0a258835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "0o7CJklAEPj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ModifiedResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(32, 32, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 64, 4, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 128, 4, stride=2)\n",
        "        self.layer4 = self._make_layer(128, 256, 3, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = ModifiedResNet()\n"
      ],
      "metadata": {
        "id": "4s9eCJ1PEh0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "M4_hTnDhEkwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "metadata": {
        "id": "ajzCqgdoEqmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cutout import Cutout\n",
        "\n",
        "transform_train.transforms.append(Cutout(n_holes=1, length=16))\n"
      ],
      "metadata": {
        "id": "BeWdTmGUErU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtEYyA38Fo8q",
        "outputId": "cb936ed2-8a06-453d-c791-a36a7220709f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 106026419.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above in the model description:\n",
        "\n",
        "Learning rate (lr): 0.001 & Weight decay: 0.0001"
      ],
      "metadata": {
        "id": "oOX9mR5jq7U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModifiedResNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "dZg4cHBuFrl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-5)\n"
      ],
      "metadata": {
        "id": "D6fgOaqaFtdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 150\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss/(i+1)}, Train accuracy: {100*correct/total}\")\n",
        "\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "id": "SshqPMlHFxRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cade2b-f395-4aec-b2a4-7d131e2e5bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.696396364885218, Train accuracy: 37.144\n",
            "Epoch: 2, Loss: 1.3318028007931721, Train accuracy: 51.562\n",
            "Epoch: 3, Loss: 1.158014152360999, Train accuracy: 58.414\n",
            "Epoch: 4, Loss: 1.0519595659907213, Train accuracy: 62.406\n",
            "Epoch: 5, Loss: 0.9651200972554629, Train accuracy: 65.498\n",
            "Epoch: 6, Loss: 0.8982390179048718, Train accuracy: 68.108\n",
            "Epoch: 7, Loss: 0.8393398878519492, Train accuracy: 70.44\n",
            "Epoch: 8, Loss: 0.7930788335287967, Train accuracy: 72.034\n",
            "Epoch: 9, Loss: 0.7572673431137944, Train accuracy: 73.454\n",
            "Epoch: 10, Loss: 0.7176882324316313, Train accuracy: 74.694\n",
            "Epoch: 11, Loss: 0.686328514000339, Train accuracy: 75.788\n",
            "Epoch: 12, Loss: 0.6657198053949019, Train accuracy: 76.646\n",
            "Epoch: 13, Loss: 0.6383969003282239, Train accuracy: 77.64\n",
            "Epoch: 14, Loss: 0.6204934155239779, Train accuracy: 78.11\n",
            "Epoch: 15, Loss: 0.5999529958533509, Train accuracy: 78.944\n",
            "Epoch: 16, Loss: 0.583392344624795, Train accuracy: 79.486\n",
            "Epoch: 17, Loss: 0.5656560574803511, Train accuracy: 80.162\n",
            "Epoch: 18, Loss: 0.5467571050614652, Train accuracy: 80.96\n",
            "Epoch: 19, Loss: 0.5365527199052483, Train accuracy: 81.468\n",
            "Epoch: 20, Loss: 0.5146085789898778, Train accuracy: 81.958\n",
            "Epoch: 21, Loss: 0.5078753265730865, Train accuracy: 82.206\n",
            "Epoch: 22, Loss: 0.49719115420985405, Train accuracy: 82.544\n",
            "Epoch: 23, Loss: 0.48374260744780223, Train accuracy: 82.99\n",
            "Epoch: 24, Loss: 0.4719821540900813, Train accuracy: 83.376\n",
            "Epoch: 25, Loss: 0.46068735607444783, Train accuracy: 83.626\n",
            "Epoch: 26, Loss: 0.44722765855624547, Train accuracy: 84.24\n",
            "Epoch: 27, Loss: 0.44154746552257584, Train accuracy: 84.448\n",
            "Epoch: 28, Loss: 0.43639089247149887, Train accuracy: 84.81\n",
            "Epoch: 29, Loss: 0.4246612264372199, Train accuracy: 85.1\n",
            "Epoch: 30, Loss: 0.4164117073159084, Train accuracy: 85.294\n",
            "Epoch: 31, Loss: 0.41293973565254066, Train accuracy: 85.568\n",
            "Epoch: 32, Loss: 0.40133662944864434, Train accuracy: 85.812\n",
            "Epoch: 33, Loss: 0.3926681227162671, Train accuracy: 86.278\n",
            "Epoch: 34, Loss: 0.38207588765932166, Train accuracy: 86.674\n",
            "Epoch: 35, Loss: 0.37587510975425503, Train accuracy: 86.618\n",
            "Epoch: 36, Loss: 0.369413030193285, Train accuracy: 86.866\n",
            "Epoch: 37, Loss: 0.3647740468420946, Train accuracy: 87.078\n",
            "Epoch: 38, Loss: 0.35494408282020207, Train accuracy: 87.47\n",
            "Epoch: 39, Loss: 0.3476526393457447, Train accuracy: 87.778\n",
            "Epoch: 40, Loss: 0.3439537142320057, Train accuracy: 87.876\n",
            "Epoch: 41, Loss: 0.3366858732822301, Train accuracy: 88.24\n",
            "Epoch: 42, Loss: 0.33105579186278533, Train accuracy: 88.308\n",
            "Epoch: 43, Loss: 0.3215599611896993, Train accuracy: 88.654\n",
            "Epoch: 44, Loss: 0.3200459893402236, Train accuracy: 88.662\n",
            "Epoch: 45, Loss: 0.31222821737799195, Train accuracy: 88.876\n",
            "Epoch: 46, Loss: 0.3064021242930151, Train accuracy: 89.24\n",
            "Epoch: 47, Loss: 0.30136875202283836, Train accuracy: 89.278\n",
            "Epoch: 48, Loss: 0.28882962469097295, Train accuracy: 89.832\n",
            "Epoch: 49, Loss: 0.2893871565342254, Train accuracy: 89.954\n",
            "Epoch: 50, Loss: 0.2894126224281538, Train accuracy: 89.834\n",
            "Epoch: 51, Loss: 0.2786393874822675, Train accuracy: 90.018\n",
            "Epoch: 52, Loss: 0.2717272610310703, Train accuracy: 90.33\n",
            "Epoch: 53, Loss: 0.2674561431035971, Train accuracy: 90.524\n",
            "Epoch: 54, Loss: 0.2644264380759595, Train accuracy: 90.6\n",
            "Epoch: 55, Loss: 0.25757670796969356, Train accuracy: 90.722\n",
            "Epoch: 56, Loss: 0.2552333406513304, Train accuracy: 90.906\n",
            "Epoch: 57, Loss: 0.2542382353711921, Train accuracy: 90.872\n",
            "Epoch: 58, Loss: 0.2393523485344999, Train accuracy: 91.434\n",
            "Epoch: 59, Loss: 0.2414581422000895, Train accuracy: 91.394\n",
            "Epoch: 60, Loss: 0.23786720799286956, Train accuracy: 91.494\n",
            "Epoch: 61, Loss: 0.23461350253628344, Train accuracy: 91.622\n",
            "Epoch: 62, Loss: 0.22658364476678927, Train accuracy: 91.942\n",
            "Epoch: 63, Loss: 0.22314355684363324, Train accuracy: 92.126\n",
            "Epoch: 64, Loss: 0.22385378037114886, Train accuracy: 92.042\n",
            "Epoch: 65, Loss: 0.2185242259136551, Train accuracy: 92.206\n",
            "Epoch: 66, Loss: 0.21361503974937113, Train accuracy: 92.404\n",
            "Epoch: 67, Loss: 0.21231482029342286, Train accuracy: 92.318\n",
            "Epoch: 68, Loss: 0.20839007205479895, Train accuracy: 92.652\n",
            "Epoch: 69, Loss: 0.20401648163338146, Train accuracy: 92.744\n",
            "Epoch: 70, Loss: 0.198427995490601, Train accuracy: 92.86\n",
            "Epoch: 71, Loss: 0.1983117798481451, Train accuracy: 92.926\n",
            "Epoch: 72, Loss: 0.19344771521932938, Train accuracy: 93.1\n",
            "Epoch: 73, Loss: 0.19214568418615005, Train accuracy: 93.208\n",
            "Epoch: 74, Loss: 0.1887391962091941, Train accuracy: 93.226\n",
            "Epoch: 75, Loss: 0.18391063037659505, Train accuracy: 93.354\n",
            "Epoch: 76, Loss: 0.18053837757929206, Train accuracy: 93.448\n",
            "Epoch: 77, Loss: 0.17800201138343347, Train accuracy: 93.762\n",
            "Epoch: 78, Loss: 0.17807234257764523, Train accuracy: 93.686\n",
            "Epoch: 79, Loss: 0.17665475543083436, Train accuracy: 93.726\n",
            "Epoch: 80, Loss: 0.17414730888269747, Train accuracy: 93.776\n",
            "Epoch: 81, Loss: 0.17357376760915114, Train accuracy: 93.804\n",
            "Epoch: 82, Loss: 0.16658946927017568, Train accuracy: 94.034\n",
            "Epoch: 83, Loss: 0.16520958755860854, Train accuracy: 94.02\n",
            "Epoch: 84, Loss: 0.16656457426031226, Train accuracy: 94.144\n",
            "Epoch: 85, Loss: 0.16218197244741117, Train accuracy: 94.332\n",
            "Epoch: 86, Loss: 0.16430850235549996, Train accuracy: 94.168\n",
            "Epoch: 87, Loss: 0.1618385747875399, Train accuracy: 94.264\n",
            "Epoch: 88, Loss: 0.16295101703204157, Train accuracy: 94.172\n",
            "Epoch: 89, Loss: 0.1601437389221795, Train accuracy: 94.326\n",
            "Epoch: 90, Loss: 0.16144881472753747, Train accuracy: 94.22\n",
            "Epoch: 91, Loss: 0.1555124195125859, Train accuracy: 94.5\n",
            "Epoch: 92, Loss: 0.15975537737045448, Train accuracy: 94.358\n",
            "Epoch: 93, Loss: 0.15586612512693382, Train accuracy: 94.498\n",
            "Epoch: 94, Loss: 0.15601839412889823, Train accuracy: 94.358\n",
            "Epoch: 95, Loss: 0.1525609991930025, Train accuracy: 94.546\n",
            "Epoch: 96, Loss: 0.15257639202582257, Train accuracy: 94.676\n",
            "Epoch: 97, Loss: 0.1519674405436534, Train accuracy: 94.584\n",
            "Epoch: 98, Loss: 0.14988485201621604, Train accuracy: 94.654\n",
            "Epoch: 99, Loss: 0.15006139953537365, Train accuracy: 94.608\n",
            "Epoch: 100, Loss: 0.1525136471899879, Train accuracy: 94.51\n",
            "Epoch: 101, Loss: 0.15305288421833302, Train accuracy: 94.56\n",
            "Epoch: 102, Loss: 0.1489358828653155, Train accuracy: 94.728\n",
            "Epoch: 103, Loss: 0.15438777299793174, Train accuracy: 94.526\n",
            "Epoch: 104, Loss: 0.1537097079388778, Train accuracy: 94.498\n",
            "Epoch: 105, Loss: 0.15148317636659994, Train accuracy: 94.62\n",
            "Epoch: 106, Loss: 0.15236399282732277, Train accuracy: 94.614\n",
            "Epoch: 107, Loss: 0.15407384423267506, Train accuracy: 94.52\n",
            "Epoch: 108, Loss: 0.15216267977834053, Train accuracy: 94.57\n",
            "Epoch: 109, Loss: 0.15012924066361258, Train accuracy: 94.636\n",
            "Epoch: 110, Loss: 0.15476965163941578, Train accuracy: 94.436\n",
            "Epoch: 111, Loss: 0.151450451272909, Train accuracy: 94.592\n",
            "Epoch: 112, Loss: 0.14976092622331952, Train accuracy: 94.654\n",
            "Epoch: 113, Loss: 0.15100409744112084, Train accuracy: 94.568\n",
            "Epoch: 114, Loss: 0.15275087373335952, Train accuracy: 94.632\n",
            "Epoch: 115, Loss: 0.15337602033868164, Train accuracy: 94.53\n",
            "Epoch: 116, Loss: 0.15690867370351805, Train accuracy: 94.344\n",
            "Epoch: 117, Loss: 0.15480515126453337, Train accuracy: 94.436\n",
            "Epoch: 118, Loss: 0.15556058096115852, Train accuracy: 94.48\n",
            "Epoch: 119, Loss: 0.15359336691324973, Train accuracy: 94.602\n",
            "Epoch: 120, Loss: 0.15830342515426524, Train accuracy: 94.33\n",
            "Epoch: 121, Loss: 0.15911132066755954, Train accuracy: 94.294\n",
            "Epoch: 122, Loss: 0.15799929775164256, Train accuracy: 94.372\n",
            "Epoch: 123, Loss: 0.15870262311814387, Train accuracy: 94.394\n",
            "Epoch: 124, Loss: 0.15721042064564003, Train accuracy: 94.414\n",
            "Epoch: 125, Loss: 0.1564782108930523, Train accuracy: 94.374\n",
            "Epoch: 126, Loss: 0.15850847707989882, Train accuracy: 94.386\n",
            "Epoch: 127, Loss: 0.16387384226712423, Train accuracy: 94.234\n",
            "Epoch: 128, Loss: 0.16214889065002847, Train accuracy: 94.152\n",
            "Epoch: 129, Loss: 0.1626141635543855, Train accuracy: 94.236\n",
            "Epoch: 130, Loss: 0.16090196026179493, Train accuracy: 94.232\n",
            "Epoch: 131, Loss: 0.1651688473456351, Train accuracy: 94.122\n",
            "Epoch: 132, Loss: 0.1649195444591515, Train accuracy: 94.118\n",
            "Epoch: 133, Loss: 0.17088322431001518, Train accuracy: 93.852\n",
            "Epoch: 134, Loss: 0.17072100532443626, Train accuracy: 93.84\n",
            "Epoch: 135, Loss: 0.17247701682093197, Train accuracy: 93.836\n",
            "Epoch: 136, Loss: 0.17509723282264322, Train accuracy: 93.736\n",
            "Epoch: 137, Loss: 0.17538985427078382, Train accuracy: 93.802\n",
            "Epoch: 138, Loss: 0.1779834523873256, Train accuracy: 93.742\n",
            "Epoch: 139, Loss: 0.17918365725013605, Train accuracy: 93.48\n",
            "Epoch: 140, Loss: 0.17662235391338157, Train accuracy: 93.792\n",
            "Epoch: 141, Loss: 0.1753889072657851, Train accuracy: 93.76\n",
            "Epoch: 142, Loss: 0.17839856845948399, Train accuracy: 93.732\n",
            "Epoch: 143, Loss: 0.18135818978176094, Train accuracy: 93.504\n",
            "Epoch: 144, Loss: 0.18608364039827185, Train accuracy: 93.43\n",
            "Epoch: 145, Loss: 0.18515918639195544, Train accuracy: 93.542\n",
            "Epoch: 146, Loss: 0.18570516725330402, Train accuracy: 93.444\n",
            "Epoch: 147, Loss: 0.18425977430151552, Train accuracy: 93.336\n",
            "Epoch: 148, Loss: 0.1878074162908832, Train accuracy: 93.258\n",
            "Epoch: 149, Loss: 0.18914659355607483, Train accuracy: 93.31\n",
            "Epoch: 150, Loss: 0.19282423756311617, Train accuracy: 93.148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Test accuracy: {100*correct/total}\")\n"
      ],
      "metadata": {
        "id": "_w5N9orQF0sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2192a1-49f2-4640-ef46-694385b157ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 92.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "The final model architecture achieved a test accuracy of 92.78%, demonstrating the effectiveness of our design choices and optimization techniques for improving the performance on the CIFAR-10 dataset.\n"
      ],
      "metadata": {
        "id": "XU-uhlURqibc"
      }
    }
  ]
}