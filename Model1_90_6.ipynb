{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1:**\n",
        "\n",
        "For our First Model we are using Stochastic Gradient Descent (SGD) as the optimizer. The learning rate, momentum, and regularization weight decay values are as follows:\n",
        "\n",
        "Learning rate (lr): 0.1, Momentum: 0.9, Regularization weight decay: 0.0001\n",
        "\n",
        "A learning rate scheduler is also used, which adjusts the learning rate during training. The milestones for the scheduler are set at epochs 30, 60, and 90, and the learning rate is multiplied by a factor of 0.1 (gamma) at each milestone. \n"
      ],
      "metadata": {
        "id": "XBV0cvuvocr_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ECrftl4Ay_Z"
      },
      "source": [
        "Installing required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD6JqeIPW9xM",
        "outputId": "47b4775d-ed4f-41ed-b82b-7d61d06d1d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_euEG1gA1SR"
      },
      "source": [
        "Importing necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBexhoYqajn7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yne28t7ZA5Hg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_Qd5WpDaw0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ModifiedResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModifiedResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(32, 32, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 64, 3, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 128, 3, stride=2)\n",
        "        self.layer4 = self._make_layer(128, 256, 3, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = ModifiedResNet()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-AwprDBCeO"
      },
      "source": [
        "Defining the ModifiedResNet and BasicBlock classes (use the code provided in the previous response).\n",
        "Set up the device (use GPU if available):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I5cE7PNbFpo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB42iSUlBJj4"
      },
      "source": [
        "Defining data augmentation and normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMWHjCmPa2PE"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWKMRnmIBHv-"
      },
      "source": [
        "Loading the CIFAR-10 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE7mzt-ca4cx",
        "outputId": "1970a75c-e7b5-4b18-ef7a-7a0ac23a1950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13098455.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTTikGVSBPdA"
      },
      "source": [
        "Instantiating the model, loss function, and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkULXL_Ta_iJ"
      },
      "outputs": [],
      "source": [
        "model = ModifiedResNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POV8C8BTBR1z"
      },
      "source": [
        "Defining the learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6Ta6gMLAiZ2"
      },
      "outputs": [],
      "source": [
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMB4dxBwBdt_"
      },
      "source": [
        "Add a new variable to track the best validation loss and create a counter to keep track of epochs with no improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GKqXWTNZE_u1"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "patience = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qqsm2mYBgZT"
      },
      "source": [
        "Splitting the training data into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQfbbjFrFB_F"
      },
      "outputs": [],
      "source": [
        "val_size = 0.1\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [int((1-val_size)*len(trainset)), int(val_size*len(trainset))])\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(valset, batch_size=100, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7IKP6tRBsqn"
      },
      "source": [
        "We are going to use the torchsummary.summary function after defining the model and before the training loop to check the number of parameters in your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzevwdJ6G5Kk",
        "outputId": "eb13b074-186a-4e99-f318-55fd8bc97558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.9/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "pip install torchsummary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSvmtu5-BxDv"
      },
      "source": [
        "Then, importing the torchsummary module in your Python script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQIWMJ76G716"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTrRTdcPB8GO"
      },
      "source": [
        "checking the number of parameters using torchsummary.summary, and defining the loss function and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meWYfmK_HG4d",
        "outputId": "8f2f2ad7-709e-4dbe-946e-d79034ba0152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-10           [-1, 32, 32, 32]               0\n",
            "           Conv2d-11           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
            "             ReLU-13           [-1, 32, 32, 32]               0\n",
            "           Conv2d-14           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-15           [-1, 32, 32, 32]              64\n",
            "             ReLU-16           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-17           [-1, 32, 32, 32]               0\n",
            "           Conv2d-18           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-19           [-1, 32, 32, 32]              64\n",
            "             ReLU-20           [-1, 32, 32, 32]               0\n",
            "           Conv2d-21           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-22           [-1, 32, 32, 32]              64\n",
            "             ReLU-23           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-24           [-1, 32, 32, 32]               0\n",
            "           Conv2d-25           [-1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-26           [-1, 64, 16, 16]             128\n",
            "             ReLU-27           [-1, 64, 16, 16]               0\n",
            "           Conv2d-28           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-29           [-1, 64, 16, 16]             128\n",
            "           Conv2d-30           [-1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
            "             ReLU-32           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-33           [-1, 64, 16, 16]               0\n",
            "           Conv2d-34           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-35           [-1, 64, 16, 16]             128\n",
            "             ReLU-36           [-1, 64, 16, 16]               0\n",
            "           Conv2d-37           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-38           [-1, 64, 16, 16]             128\n",
            "             ReLU-39           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-40           [-1, 64, 16, 16]               0\n",
            "           Conv2d-41           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-42           [-1, 64, 16, 16]             128\n",
            "             ReLU-43           [-1, 64, 16, 16]               0\n",
            "           Conv2d-44           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-45           [-1, 64, 16, 16]             128\n",
            "             ReLU-46           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-47           [-1, 64, 16, 16]               0\n",
            "           Conv2d-48            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-49            [-1, 128, 8, 8]             256\n",
            "             ReLU-50            [-1, 128, 8, 8]               0\n",
            "           Conv2d-51            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-52            [-1, 128, 8, 8]             256\n",
            "           Conv2d-53            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-54            [-1, 128, 8, 8]             256\n",
            "             ReLU-55            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-56            [-1, 128, 8, 8]               0\n",
            "           Conv2d-57            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-58            [-1, 128, 8, 8]             256\n",
            "             ReLU-59            [-1, 128, 8, 8]               0\n",
            "           Conv2d-60            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-61            [-1, 128, 8, 8]             256\n",
            "             ReLU-62            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-63            [-1, 128, 8, 8]               0\n",
            "           Conv2d-64            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-65            [-1, 128, 8, 8]             256\n",
            "             ReLU-66            [-1, 128, 8, 8]               0\n",
            "           Conv2d-67            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-68            [-1, 128, 8, 8]             256\n",
            "             ReLU-69            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-70            [-1, 128, 8, 8]               0\n",
            "           Conv2d-71            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-72            [-1, 256, 4, 4]             512\n",
            "             ReLU-73            [-1, 256, 4, 4]               0\n",
            "           Conv2d-74            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-75            [-1, 256, 4, 4]             512\n",
            "           Conv2d-76            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
            "             ReLU-78            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-79            [-1, 256, 4, 4]               0\n",
            "           Conv2d-80            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-81            [-1, 256, 4, 4]             512\n",
            "             ReLU-82            [-1, 256, 4, 4]               0\n",
            "           Conv2d-83            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-84            [-1, 256, 4, 4]             512\n",
            "             ReLU-85            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-86            [-1, 256, 4, 4]               0\n",
            "           Conv2d-87            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-88            [-1, 256, 4, 4]             512\n",
            "             ReLU-89            [-1, 256, 4, 4]               0\n",
            "           Conv2d-90            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-91            [-1, 256, 4, 4]             512\n",
            "             ReLU-92            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-93            [-1, 256, 4, 4]               0\n",
            "AdaptiveAvgPool2d-94            [-1, 256, 1, 1]               0\n",
            "           Linear-95                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,366,250\n",
            "Trainable params: 4,366,250\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 11.03\n",
            "Params size (MB): 16.66\n",
            "Estimated Total Size (MB): 27.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = ModifiedResNet().to(device)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FceflPmBXNB"
      },
      "source": [
        "Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWnREkZLAE-u",
        "outputId": "40cc8b77-5d24-4749-c52e-e2151421bc3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 2.085784676738761, Train accuracy: 23.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Loss: 1.6759634840894828, Train accuracy: 37.45777777777778\n",
            "Epoch: 3, Loss: 1.4603323939849029, Train accuracy: 46.522222222222226\n",
            "Epoch: 4, Loss: 1.2505592922256752, Train accuracy: 54.84444444444444\n",
            "Epoch: 5, Loss: 1.10368228669871, Train accuracy: 60.611111111111114\n",
            "Epoch: 6, Loss: 1.003720301118764, Train accuracy: 64.41777777777777\n",
            "Epoch: 7, Loss: 0.9114242409440604, Train accuracy: 67.75111111111111\n",
            "Epoch: 8, Loss: 0.8278325725008141, Train accuracy: 70.73555555555555\n",
            "Epoch: 9, Loss: 0.7670673512938347, Train accuracy: 72.91333333333333\n",
            "Epoch: 10, Loss: 0.7081148137592457, Train accuracy: 75.42\n",
            "Epoch: 11, Loss: 0.6673356741666794, Train accuracy: 76.67333333333333\n",
            "Epoch: 12, Loss: 0.6374069205061956, Train accuracy: 77.80888888888889\n",
            "Epoch: 13, Loss: 0.6088407468897375, Train accuracy: 78.82888888888888\n",
            "Epoch: 14, Loss: 0.5844868937168609, Train accuracy: 79.72666666666667\n",
            "Epoch: 15, Loss: 0.5577307285910303, Train accuracy: 80.58444444444444\n",
            "Epoch: 16, Loss: 0.5407064121047204, Train accuracy: 81.25777777777778\n",
            "Epoch: 17, Loss: 0.5132802022113041, Train accuracy: 82.23333333333333\n",
            "Epoch: 18, Loss: 0.5029763847758825, Train accuracy: 82.61555555555556\n",
            "Epoch: 19, Loss: 0.48361601219089195, Train accuracy: 83.1\n",
            "Epoch: 20, Loss: 0.47087380256165157, Train accuracy: 83.7311111111111\n",
            "Epoch: 21, Loss: 0.45419011502103374, Train accuracy: 84.06\n",
            "Epoch: 22, Loss: 0.44478445072573697, Train accuracy: 84.51555555555555\n",
            "Epoch: 23, Loss: 0.43548778173598374, Train accuracy: 84.89555555555556\n",
            "Epoch: 24, Loss: 0.4264131497845731, Train accuracy: 85.17111111111112\n",
            "Epoch: 25, Loss: 0.41682140939784323, Train accuracy: 85.40444444444445\n",
            "Epoch: 26, Loss: 0.41328105770728807, Train accuracy: 85.63777777777777\n",
            "Epoch: 27, Loss: 0.4028822214511985, Train accuracy: 86.14888888888889\n",
            "Epoch: 28, Loss: 0.39399911247363145, Train accuracy: 86.38\n",
            "Epoch: 29, Loss: 0.388740547509356, Train accuracy: 86.39777777777778\n",
            "Epoch: 30, Loss: 0.3821064880625768, Train accuracy: 86.69777777777777\n",
            "Epoch: 31, Loss: 0.372501235878603, Train accuracy: 86.95111111111112\n",
            "Epoch: 32, Loss: 0.367128419986164, Train accuracy: 87.11333333333333\n",
            "Epoch: 33, Loss: 0.36207925337790087, Train accuracy: 87.41777777777777\n",
            "Epoch: 34, Loss: 0.3557801135731014, Train accuracy: 87.73555555555555\n",
            "Epoch: 35, Loss: 0.3528828846544705, Train accuracy: 87.60666666666667\n",
            "Epoch: 36, Loss: 0.35121087975461374, Train accuracy: 87.78\n",
            "Epoch: 37, Loss: 0.34496523774313653, Train accuracy: 87.94666666666667\n",
            "Epoch: 38, Loss: 0.3384844970212064, Train accuracy: 88.27111111111111\n",
            "Epoch: 39, Loss: 0.3371267668818208, Train accuracy: 88.18666666666667\n",
            "Epoch: 40, Loss: 0.3305614000101658, Train accuracy: 88.46222222222222\n",
            "Epoch: 41, Loss: 0.32064208007332956, Train accuracy: 88.8488888888889\n",
            "Epoch: 42, Loss: 0.3220561788502065, Train accuracy: 88.86666666666666\n",
            "Epoch: 43, Loss: 0.31935943087393587, Train accuracy: 88.99777777777778\n",
            "Epoch: 44, Loss: 0.31963144475594163, Train accuracy: 88.82444444444444\n",
            "Epoch: 45, Loss: 0.3143700042062185, Train accuracy: 88.92444444444445\n",
            "Epoch: 46, Loss: 0.30877338303253055, Train accuracy: 89.13111111111111\n",
            "Epoch: 47, Loss: 0.3094182149930434, Train accuracy: 89.20444444444445\n",
            "Epoch: 48, Loss: 0.3058452023701234, Train accuracy: 89.35111111111111\n",
            "Epoch: 49, Loss: 0.3039481869729405, Train accuracy: 89.37555555555555\n",
            "Epoch: 50, Loss: 0.29880169131369755, Train accuracy: 89.58\n",
            "Epoch: 51, Loss: 0.2987953785552897, Train accuracy: 89.57777777777778\n",
            "Epoch: 52, Loss: 0.29484629398211837, Train accuracy: 89.70666666666666\n",
            "Epoch: 53, Loss: 0.2886850068938326, Train accuracy: 89.89111111111112\n",
            "Epoch: 54, Loss: 0.29140911977314815, Train accuracy: 89.90444444444445\n",
            "Epoch: 55, Loss: 0.2881074025380341, Train accuracy: 89.88222222222223\n",
            "Epoch: 56, Loss: 0.28503927279433067, Train accuracy: 90.00666666666666\n",
            "Epoch: 57, Loss: 0.28251539496704936, Train accuracy: 90.06888888888889\n",
            "Epoch: 58, Loss: 0.2841515941333703, Train accuracy: 89.9888888888889\n",
            "Epoch: 59, Loss: 0.27907477503388445, Train accuracy: 90.16\n",
            "Epoch: 60, Loss: 0.274462446147068, Train accuracy: 90.19111111111111\n",
            "Epoch: 61, Loss: 0.28344653447327967, Train accuracy: 90.08666666666667\n",
            "Epoch: 62, Loss: 0.26677523930133745, Train accuracy: 90.65333333333334\n",
            "Epoch: 63, Loss: 0.27215568827126513, Train accuracy: 90.38222222222223\n",
            "Epoch: 64, Loss: 0.2729775412431495, Train accuracy: 90.45111111111112\n",
            "Epoch: 65, Loss: 0.27132073703052645, Train accuracy: 90.53111111111112\n",
            "Epoch: 66, Loss: 0.26848826299167494, Train accuracy: 90.69111111111111\n",
            "Epoch: 67, Loss: 0.26132826905020257, Train accuracy: 90.7911111111111\n",
            "Epoch: 68, Loss: 0.2700818979012018, Train accuracy: 90.55777777777777\n",
            "Epoch: 69, Loss: 0.2618642948694866, Train accuracy: 90.88\n",
            "Epoch: 70, Loss: 0.2654566979247399, Train accuracy: 90.76444444444445\n",
            "Epoch: 71, Loss: 0.26576237636618316, Train accuracy: 90.67333333333333\n",
            "Epoch: 72, Loss: 0.25781697648662055, Train accuracy: 90.82888888888888\n",
            "Epoch: 73, Loss: 0.2610859134530818, Train accuracy: 90.80222222222223\n",
            "Epoch: 74, Loss: 0.2610083627514541, Train accuracy: 90.81111111111112\n",
            "Epoch: 75, Loss: 0.2499662570613013, Train accuracy: 91.24888888888889\n",
            "Epoch: 76, Loss: 0.26399398485029285, Train accuracy: 90.71555555555555\n",
            "Epoch: 77, Loss: 0.25025312862866983, Train accuracy: 91.25333333333333\n",
            "Epoch: 78, Loss: 0.25350131356919353, Train accuracy: 91.00444444444445\n",
            "Epoch: 79, Loss: 0.2515666683797132, Train accuracy: 91.20666666666666\n",
            "Epoch: 80, Loss: 0.25134156413630326, Train accuracy: 91.20444444444445\n",
            "Epoch: 81, Loss: 0.25501653949984093, Train accuracy: 91.0911111111111\n",
            "Epoch: 82, Loss: 0.25000356903977017, Train accuracy: 91.08222222222223\n",
            "Epoch: 83, Loss: 0.24700725722041997, Train accuracy: 91.36666666666666\n",
            "Epoch: 84, Loss: 0.24941447008909148, Train accuracy: 91.2688888888889\n",
            "Epoch: 85, Loss: 0.250074948776852, Train accuracy: 91.33111111111111\n",
            "Epoch: 86, Loss: 0.2436175396327268, Train accuracy: 91.31555555555556\n",
            "Epoch: 87, Loss: 0.24980177144011992, Train accuracy: 91.19555555555556\n",
            "Epoch: 88, Loss: 0.24248210435987197, Train accuracy: 91.43333333333334\n",
            "Epoch: 89, Loss: 0.2426264913202348, Train accuracy: 91.51777777777778\n",
            "Epoch: 90, Loss: 0.24572649435140193, Train accuracy: 91.43555555555555\n",
            "Epoch: 91, Loss: 0.23707369300113482, Train accuracy: 91.67777777777778\n",
            "Epoch: 92, Loss: 0.24277624712240967, Train accuracy: 91.49333333333334\n",
            "Epoch: 93, Loss: 0.2377494702919979, Train accuracy: 91.58666666666667\n",
            "Epoch: 94, Loss: 0.24354606661522252, Train accuracy: 91.45333333333333\n",
            "Epoch: 95, Loss: 0.2395697057204829, Train accuracy: 91.65555555555555\n",
            "Epoch: 96, Loss: 0.24018926738592034, Train accuracy: 91.56888888888889\n",
            "Epoch: 97, Loss: 0.23843322615985843, Train accuracy: 91.77777777777777\n",
            "Epoch: 98, Loss: 0.2321274141484702, Train accuracy: 91.86888888888889\n",
            "Epoch: 99, Loss: 0.2320730123372579, Train accuracy: 91.86\n",
            "Epoch: 100, Loss: 0.2376177295280451, Train accuracy: 91.68666666666667\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {running_loss/(i+1)}, Train accuracy: {100*correct/total}\")\n",
        "\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3FczkBQBZtd"
      },
      "source": [
        "Testing the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyiZybKCc0YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65561e65-252d-4c03-c162-2626b10bdec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 90.6\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Test accuracy: {100*correct/total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "The final model architecture achieved a test accuracy of 90.6%, demonstrating the effectiveness of our design choices and optimization techniques for improving the performance on the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "Wqjyv-MupDFi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYv--gHepI7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}